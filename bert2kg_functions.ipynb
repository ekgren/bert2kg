{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import transformers\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = transformers.AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')\n",
    "model = transformers.AutoModel.from_pretrained('KB/bert-base-swedish-cased').eval()\n",
    "nlp = spacy.load('../data/sv_model_xpos/sv_model0/sv_model0-0.0.0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT=\"\"\"Bob Dylan föddes som Robert Zimmerman i staden Duluth, Minnesota men strax innan han fyllde sex år och efter att hans far fått polio flyttade familjen till den närliggande staden Hibbing, Minnesota där han sedan växte upp. Familjen Zimmerman var judisk och deras förfäder hade utvandrat från Ryssland, Ukraina, Litauen och Turkiet. Morfar och mormor - Benjamin och Liba Edelstein (senare Stein och Stone) - var litauiska judar som emigrerade till USA 1902.\n",
    "När Bob Dylan var åtta-nio år började han spela på familjens piano. Därefter lärde han sig att spela munspel och gitarr.[3] Mycket av hans ungdomstid gick åt till att lyssna på radio där han tog in stationer som sände blues, country och tidig rock'n'roll. Han började uppträda i mitten av 1950-talet och var medlem i ett flertal band under sin tid i high school.\n",
    "\n",
    "1959 började han studera på universitetet i Minneapolis. I samma veva tog hans intresse för folkmusik fart. Det var också nu han började presentera sig som Bob Dylan. Var han fått namnet ifrån finns det flera historier om. Vissa menar att det är inspirerat av poeten Dylan Thomas. År 2004 skrev han själv om hur han valde namnet i sin bok Memoarer, första delen\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    length = 0\n",
    "    input_ids = []\n",
    "    word_start = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    ixs, tokens = zip(*[(ix, token) for (ix, token) in enumerate(doc) if not token.is_space])\n",
    "    \n",
    "    wordpieces_for_token = tok(\n",
    "            [token.string for token in tokens],\n",
    "            add_special_tokens=False, \n",
    "            padding=False, \n",
    "            return_token_type_ids=False, \n",
    "            return_attention_mask=False)['input_ids']\n",
    "    \n",
    "    for wordpieces in wordpieces_for_token:\n",
    "        assert len(wordpieces) > 0, \"Empty token makes program sad\"\n",
    "        word_start.append(length)\n",
    "        input_ids += wordpieces\n",
    "        length += len(wordpieces)\n",
    "    \n",
    "    nouns = []\n",
    "    spans = []\n",
    "    ptr = 0\n",
    "    \n",
    "    chunkfix = {i:j for j,i in enumerate(ixs)}\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "        start = chunkfix[chunk.start]\n",
    "        end = chunkfix[chunk.end]\n",
    "        #Add the next chunk to spans        \n",
    "        for i in range(ptr, start):\n",
    "            #Add all non noun chunks to the span\n",
    "            nouns.append(False)\n",
    "            spans.append(word_start[i])\n",
    "            \n",
    "        #Add the chunk to the spans\n",
    "        nouns.append(True)\n",
    "        spans.append(word_start[start])\n",
    "        ptr = end\n",
    "        \n",
    "    #Add trailing (non noun) chunks to the span\n",
    "    nouns.extend([False for start in word_start[ptr:]])\n",
    "    spans.extend([start for start in word_start[ptr:]])\n",
    "    \n",
    "    # Add cls token\n",
    "    input_ids = torch.LongTensor([tok.cls_token_id, *input_ids])\n",
    "    nouns = [False] +  nouns\n",
    "    spans = [0] + [start + 1 for start in spans]\n",
    "    \n",
    "    spans = list(zip(spans, spans[1:] + [len(input_ids)]))\n",
    "    \n",
    "    return input_ids, nouns, spans\n",
    "\n",
    "input_ids, nouns, spans = parse_sentence(TXT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nlp(TXT).noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for noun, (start, stop) in zip(nouns, spans):\n",
    "    if noun: \n",
    "        print(tok.decode(input_ids[start:stop]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(input_ids, spans):\n",
    "    # Average attention over heads in the last layer\n",
    "    # (Using cumulative sum)\n",
    "    att = model(input_ids.unsqueeze(0), output_attentions=True)['attentions'][-1].mean(1).squeeze(0)\n",
    "    csatt = att.cumsum(0).cumsum(1)\n",
    "\n",
    "    starts, ends = zip(*spans)\n",
    "    starts = torch.LongTensor(starts) - 1\n",
    "    ends = torch.LongTensor(ends) - 1\n",
    "    \n",
    "    ret = csatt[starts, :][:, starts] - csatt[starts, :][:, ends] - csatt[ends, :][:, starts] + csatt[ends, :][:, ends] \n",
    "    ret[0,:] = csatt[0,ends] - csatt[0,starts]\n",
    "    ret[:,0] = csatt[ends,0] - csatt[starts,0]\n",
    "    ret[0,0] = att[0,0]\n",
    "    ret /= (ends - starts)[:, None]\n",
    "                \n",
    "    return ret\n",
    "\n",
    "att = get_attention(input_ids, spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wix(ix):\n",
    "    start, stop = spans[ix]\n",
    "    return tok.decode(input_ids[start:stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_ixs = [ix for ix, noun in enumerate(nouns) if noun]\n",
    "rel_ixs = [ix for ix, noun in enumerate(nouns) if not noun]\n",
    "rixs = torch.LongTensor(rel_ixs)\n",
    "\n",
    "\n",
    "for head, tail in itertools.product(noun_ixs, noun_ixs):\n",
    "    if abs(head - tail) <= 1:\n",
    "        continue\n",
    "        \n",
    "    rixs = torch.LongTensor([ix for ix, noun in enumerate(nouns) if (not noun) and (head < ix < tail or head > ix > tail)])\n",
    "    scores = att[head, rixs] + att[rixs, tail]\n",
    "    best = torch.argmax(scores)\n",
    "    rel = rixs[best].item()\n",
    "    score = scores[best].item()\n",
    "    print('{:.3f}: {} -- {} -- {}'.format(score, print_wix(head), print_wix(rel), print_wix(tail)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list_relation = set([ token2id[n]  for n in noun_chunks ])\n",
    "all_relation_pairs = []\n",
    "id2token = { value: key for key, value in token2id.items()}\n",
    "with Pool(10) as pool:\n",
    "    params = [  ( pair[0], pair[1], attn_graph, max(tokenid2word_mapping), black_list_relation, ) for pair in tail_head_pairs]\n",
    "    for output in pool.imap_unordered(bfs, params):\n",
    "    if len(output):\n",
    "        all_relation_pairs += [ (o, id2token) for o in output ]\n",
    "        \n",
    "triplet_text = []\n",
    "with Pool(10, global_initializer, (nlp,)) as pool:\n",
    "    for triplet in pool.imap_unordered(filter_relation_sets, all_relation_pairs):\n",
    "        if len(triplet) > 0:\n",
    "            triplet_text.append(triplet)\n",
    "return triplet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(input_ids, doc, word_ends):\n",
    "    # Average attention over heads in the last layer\n",
    "    att = model(input_ids.unsqueeze(0), output_attentions=True)['attentions'][-1].mean(1).squeeze(0)\n",
    "    \n",
    "    N = len(word_ends)\n",
    "    \n",
    "    tmp = att.new_zeros((N,N))\n",
    "\n",
    "    tmp[0,0] = att[0,0]\n",
    "    \n",
    "    for i in range(1,N):\n",
    "        istart, iend = word_ends[i-1], word_ends[i]\n",
    "        tmp[0, i] = att[0, istart:iend].sum()\n",
    "        tmp[i, 0] = att[istart:iend, 0].sum() / (iend-istart)\n",
    "    \n",
    "    for i,j in itertools.product(range(1,N),range(1,N)):\n",
    "        istart, iend = word_ends[i-1], word_ends[i]\n",
    "        jstart, jend = word_ends[j-1], word_ends[j]\n",
    "        tmp[i,j] = att[istart:iend, jstart:jend].sum() / (iend-istart)\n",
    "        \n",
    "#    cs = att.cumsum(0).cumsum(1)\n",
    "#    tmp2 = att.new_zeros((N,N))\n",
    "#\n",
    "#    tmp2[0,0] = att[0,0]\n",
    "#    \n",
    "#    for i in range(1, N):\n",
    "#        istart, iend = word_ends[i-1]-1, word_ends[i]-1\n",
    "#        jstart, jend = 0, 1\n",
    "#        tmp2[0,i] = cs[jend, iend] - cs[jstart, iend] - cs[jend, istart] + cs[jstart,istart]     \n",
    "#        tmp2[i,0] = cs[iend, jend] - cs[istart, jend] - cs[iend, jstart] + cs[istart,jstart]\n",
    "#        tmp2[i,0] /= iend - istart\n",
    "#    \n",
    "#    for i,j in itertools.product(range(1,N),range(1,N)):\n",
    "#        istart, iend = word_ends[i-1]-1, word_ends[i]-1\n",
    "#        jstart, jend = word_ends[j-1]-1, word_ends[j]-1\n",
    "#        tmp2[i,j] = cs[iend, jend] - cs[istart, jend] - cs[iend, jstart] + cs[istart,jstart]\n",
    "#        tmp2[i,j] /= iend - istart\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "att = get_attention(input_ids, doc, word_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csatt = att.cumsum(0).cumsum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 23\n",
    "j = 65\n",
    "csatt[0, j] - csatt[0, i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att[0, i:j+1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    if use_cuda:\n",
    "        for key in inputs.keys():\n",
    "        inputs[key] = inputs[key].cuda()\n",
    "        outputs = encoder(**inputs, output_attentions=True)\n",
    "trim = True\n",
    "'''\n",
    "Use average of last layer attention : page 6, section 3.1.2\n",
    "'''\n",
    "attention = process_matrix(outputs[2], avg_head=True, trim=trim, use_cuda=use_cuda)\n",
    "merged_attention = compress_attention(attention, tokenid2word_mapping)\n",
    "attn_graph = build_graph(merged_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, start, end) in chunk_spans:\n",
    "    print(x, tok.decode(input_ids[start:end]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk.start\n",
    "chunk.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence, tok_params):\n",
    "    huggingface_encoding = tok(sentence, **tok_params)\n",
    "    doc = nlp(sentence)\n",
    "    tokens = list(doc)\n",
    "    chunk2id = {}\n",
    "    start_chunk = []\n",
    "    end_chunk = []\n",
    "    noun_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        noun_chunks.append(chunk.text)\n",
    "        start_chunk.append(chunk.start)\n",
    "        end_chunk.append(chunk.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp:\n",
    "    def __init__(self, sentence):de\n",
    "        self.text = sentence\n",
    "        self.noun_chunks = [\n",
    "        a\n",
    "def parse_sentence(*args, **kwargs):\n",
    "    \"\"\" parse sentence \"\"\"\n",
    "    inputs, tokenid2word_mapping, token2id, noun_chunks  = create_mapping(sentence, return_pt=True, nlp=nlp, tokenizer=tokenizer)\n",
    "    triplets = []\n",
    "    return triplets\n",
    "\n",
    "def create_mapping(sentence, return_pt, nlp, tokenizer):\n",
    "    inputs = None\n",
    "    tokenid2word = None\n",
    "    token2id = None\n",
    "    noun_chunks = None\n",
    "    return inputs, tokenid2word_mapping, token2id, noun_chunks\n",
    "\n",
    "tokenizer = None\n",
    "encoder = None\n",
    "\n",
    "sentence = 'Bob Dylan är en gubbe som har skrivit musik.'\n",
    "\n",
    "triplets = parse_sentence(sent.text, tokenizer, encoder, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(sentence, return_pt=True, nlp=nlp, tokenizer=tokenizer):\n",
    "    return None\n",
    "\n",
    "def parse_sentence(sentence, tokenizer, encoder, nlp, use_cuda=True):\n",
    "    ''' Parse stuff '''\n",
    "    tokenizer_name = str(tokenizer.__str__)\n",
    "\n",
    "    inputs, tokenid2word_mapping, token2id, noun_chunks  = create_mapping(sentence, return_pt=True, nlp=nlp, tokenizer=tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_cuda:\n",
    "            for key in inputs.keys():\n",
    "                inputs[key] = inputs[key].cuda()\n",
    "        outputs = encoder(**inputs, output_attentions=True)\n",
    "    trim = True\n",
    "    if 'GPT2' in tokenizer_name:\n",
    "        trim  = False\n",
    "\n",
    "    ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff ''' Parse stuff '''\n",
    "    Use average of last layer attention : page 6, section 3.1.2\n",
    "    '''\n",
    "    attention = process_matrix(outputs[2], avg_head=True, trim=trim, use_cuda=use_cuda)\n",
    "\n",
    "    merged_attention = compress_attention(attention, tokenid2word_mapping)\n",
    "    attn_graph = build_graph(merged_attention)\n",
    "\n",
    "    tail_head_pairs = []\n",
    "    for head in noun_chunks:\n",
    "        for tail in noun_chunks:\n",
    "            if head != tail:\n",
    "                tail_head_pairs.append((token2id[head], token2id[tail]))\n",
    "\n",
    "    black_list_relation = set([ token2id[n]  for n in noun_chunks ])\n",
    "\n",
    "    all_relation_pairs = []\n",
    "    id2token = { value: key for key, value in token2id.items()}\n",
    "\n",
    "    with Pool(10) as pool:\n",
    "        params = [  ( pair[0], pair[1], attn_graph, max(tokenid2word_mapping), black_list_relation, ) for pair in tail_head_pairs]\n",
    "        for output in pool.imap_unordered(bfs, params):\n",
    "            if len(output):\n",
    "                all_relation_pairs += [ (o, id2token) for o in output ]\n",
    "\n",
    "    triplet_text = []\n",
    "    with Pool(10, global_initializer, (nlp,)) as pool:\n",
    "        for triplet in pool.imap_unordered(filter_relation_sets, all_relation_pairs):\n",
    "            if len(triplet) > 0:\n",
    "                triplet_text.append(triplet)\n",
    "    return triplet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "#nlp = spacy.load('da_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(x.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
