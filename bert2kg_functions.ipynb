{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import transformers\n",
    "import itertools\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = transformers.AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')\n",
    "model = transformers.AutoModel.from_pretrained('KB/bert-base-swedish-cased').eval()\n",
    "nlp = spacy.load('../data/sv_model_xpos/sv_model0/sv_model0-0.0.0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT=\"\"\"Bob Dylan föddes som Robert Zimmerman i staden Duluth, Minnesota men strax innan han fyllde sex år och efter att hans far fått polio flyttade familjen till den närliggande staden Hibbing, Minnesota där han sedan växte upp.\"\"\"\n",
    "\"\"\"\n",
    "Familjen Zimmerman var judisk och deras förfäder hade utvandrat från Ryssland, Ukraina, Litauen och Turkiet. Morfar och mormor - Benjamin och Liba Edelstein (senare Stein och Stone) - var litauiska judar som emigrerade till USA 1902.\n",
    "När Bob Dylan var åtta-nio år började han spela på familjens piano. Därefter lärde han sig att spela munspel och gitarr.[3] Mycket av hans ungdomstid gick åt till att lyssna på radio där han tog in stationer som sände blues, country och tidig rock'n'roll. Han började uppträda i mitten av 1950-talet och var medlem i ett flertal band under sin tid i high school.\n",
    "\n",
    "1959 började han studera på universitetet i Minneapolis. I samma veva tog hans intresse för folkmusik fart. Det var också nu han började presentera sig som Bob Dylan. Var han fått namnet ifrån finns det flera historier om. Vissa menar att det är inspirerat av poeten Dylan Thomas. År 2004 skrev han själv om hur han valde namnet i sin bok Memoarer, första delen\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence, nlp, tok):\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    length = 0\n",
    "    input_ids = []\n",
    "    word_start = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    ixs, tokens = zip(*[(ix, token) for (ix, token) in enumerate(doc) if not token.is_space])\n",
    "    \n",
    "    wordpieces_for_token = tok(\n",
    "            [token.string for token in tokens],\n",
    "            add_special_tokens=False, \n",
    "            padding=False, \n",
    "            return_token_type_ids=False, \n",
    "            return_attention_mask=False)['input_ids']\n",
    "    \n",
    "    for wordpieces in wordpieces_for_token:\n",
    "        assert len(wordpieces) > 0, \"Empty token makes program sad\"\n",
    "        word_start.append(length)\n",
    "        input_ids += wordpieces\n",
    "        length += len(wordpieces)\n",
    "    \n",
    "    nouns = []\n",
    "    spans = []\n",
    "    ptr = 0\n",
    "    \n",
    "    chunkfix = {i:j for j,i in enumerate(ixs)}\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "        start = chunkfix[chunk.start]\n",
    "        end = chunkfix[chunk.end]\n",
    "        #Add the next chunk to spans        \n",
    "        for i in range(ptr, start):\n",
    "            #Add all non noun chunks to the span\n",
    "            nouns.append(False)\n",
    "            spans.append(word_start[i])\n",
    "            \n",
    "        #Add the chunk to the spans\n",
    "        nouns.append(True)\n",
    "        spans.append(word_start[start])\n",
    "        ptr = end\n",
    "        \n",
    "    #Add trailing (non noun) chunks to the span\n",
    "    nouns.extend([False for start in word_start[ptr:]])\n",
    "    spans.extend([start for start in word_start[ptr:]])\n",
    "    \n",
    "    # Add cls token\n",
    "    input_ids = torch.LongTensor([tok.cls_token_id, *input_ids])\n",
    "    nouns = [False] +  nouns\n",
    "    spans = [0] + [start + 1 for start in spans]\n",
    "    \n",
    "    spans = list(zip(spans, spans[1:] + [len(input_ids)]))\n",
    "    \n",
    "    return input_ids, nouns, spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    those = [noun.string.strip() for noun in nlp(TXT).noun_chunks]\n",
    "    these = []\n",
    "    input_ids, nouns, spans = parse_sentence(TXT, nlp, tok)\n",
    "    for noun, (start, stop) in zip(nouns, spans):\n",
    "        if noun: \n",
    "            these.append(tok.decode(input_ids[start:stop]))\n",
    "    assert those == these, \"Spacy nouns does not match our nouns\"\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_attention(attention, spans):\n",
    "    csatt = attention.cumsum(0).cumsum(1)\n",
    "\n",
    "    starts, ends = zip(*spans)\n",
    "    starts = torch.LongTensor(starts) - 1\n",
    "    ends = torch.LongTensor(ends) - 1\n",
    "    \n",
    "    ret = csatt[starts, :][:, starts] - csatt[starts, :][:, ends] - csatt[ends, :][:, starts] + csatt[ends, :][:, ends] \n",
    "    ret[0,:] = csatt[0,ends] - csatt[0,starts]\n",
    "    ret[:,0] = csatt[ends,0] - csatt[starts,0]\n",
    "    ret[0,0] = attention[0,0]\n",
    "    ret /= (ends - starts)[:, None]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_provenance(model, input_ids, spans):\n",
    "     \n",
    "    attentions = model(input_ids.unsqueeze(0), output_attentions=True)['attentions']\n",
    "    \n",
    "    N = len(spans)\n",
    "    \n",
    "    ret = torch.eye(N)\n",
    "    rets = []\n",
    "    \n",
    "    for tmp in map(lambda att: compress_attention(att.mean(1).squeeze(0), spans), attentions):\n",
    "        ret = (ret + torch.eye(N)) @ tmp\n",
    "        ret /= 2\n",
    "        rets.append(ret)\n",
    "        \n",
    "    return torch.stack(rets).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention(model, input_ids, spans):\n",
    "    # Average attention over heads in the last layer\n",
    "    # (Using cumulative sum)\n",
    "    att = model(input_ids.unsqueeze(0), output_attentions=True)['attentions'][-1].mean(1).squeeze(0)\n",
    "    return compress_attention(att, spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplets(sentence, model, nlp, tok):\n",
    "    input_ids, nouns, spans = parse_sentence(sentence, nlp, tok)\n",
    "    attention = get_attention(model, input_ids, spans)\n",
    "    \n",
    "    noun_set = set([i for i, noun in enumerate(nouns) if noun])\n",
    "    rel_ixs = [i for i, noun in enumerate(nouns) if not noun]\n",
    "    att = attention\n",
    "    fwd_cache = attention.diag(1).cumsum(0)\n",
    "    bwd_cache = attention.diag(-1).cumsum(0)\n",
    "    \n",
    "    def lemmatized(start,stop=None):\n",
    "        lb = spans[start][0]\n",
    "        ub = spans[stop if stop else start][1]\n",
    "        txt = tok.decode(input_ids[lb:ub])\n",
    "        return ' '.join([token.lemma_ for token in nlp(txt)])\n",
    "        \n",
    "    def get_scores(head, tail):\n",
    "        scores = []\n",
    "        for start in range(head+1, tail):\n",
    "            if start in noun_set: continue\n",
    "            for stop in range(start, tail):\n",
    "                if stop in noun_set: break\n",
    "                \n",
    "                ### Forward attention (head reads from x, x reads from tail)\n",
    "                # Calculate internal part\n",
    "                fwd = fwd_cache[stop-1] - fwd_cache[start-1]\n",
    "                # Calculate ends\n",
    "                fwd += att[head, start] + att[stop, tail]\n",
    "                \n",
    "                ### Backward version (head writes to x, x writes to tail)\n",
    "                # Calculate internal part\n",
    "                bwd = bwd_cache[stop-1] - fwd_cache[start-1]\n",
    "                # Calculate ends\n",
    "                bwd += att[start, head] + att[tail, stop]\n",
    "        \n",
    "                score = max(fwd, bwd)\n",
    "                scores.append((score, start, stop))\n",
    "        \n",
    "        return scores\n",
    "        \n",
    "    for head, tail in itertools.product(noun_set, noun_set):\n",
    "        rels = heapq.nlargest(5, get_scores(head, tail))\n",
    "        for (score, start, stop) in rels:\n",
    "            yield (score, lemmatized(head), lemmatized(tail), lemmatized(start, stop))\n",
    "\n",
    "            \n",
    "for score, head, tail, relation in get_triplets(TXT, model, nlp, tok):\n",
    "    print('{:.3f} {} -- {} -- {}'.format(score, head, relation, tail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "black_list_relation = set([ token2id[n]  for n in noun_chunks ])\n",
    "all_relation_pairs = []\n",
    "id2token = { value: key for key, value in token2id.items()}\n",
    "with Pool(10) as pool:\n",
    "    params = [  ( pair[0], pair[1], attn_graph, max(tokenid2word_mapping), black_list_relation, ) for pair in tail_head_pairs]\n",
    "    for output in pool.imap_unordered(bfs, params):\n",
    "    if len(output):\n",
    "        all_relation_pairs += [ (o, id2token) for o in output ]\n",
    "        \n",
    "triplet_text = []\n",
    "with Pool(10, global_initializer, (nlp,)) as pool:\n",
    "    for triplet in pool.imap_unordered(filter_relation_sets, all_relation_pairs):\n",
    "        if len(triplet) > 0:\n",
    "            triplet_text.append(triplet)\n",
    "return triplet_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
