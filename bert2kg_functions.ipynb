{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_filename, 'r') as f, open(output_filename, 'w') as g:\n",
    "    for idx, line in enumerate(tqdm(f)):\n",
    "        sentence  = line.strip()\n",
    "        if len(sentence):\n",
    "            valid_triplets = []\n",
    "            for sent in nlp(sentence).sents:\n",
    "                # Match\n",
    "                for triplets in parse_sentence(sent.text, tokenizer, encoder, nlp, use_cuda=use_cuda):\n",
    "                    valid_triplets.append(triplets)\n",
    "            if len(valid_triplets) > 0:\n",
    "                # Map\n",
    "                mapped_triplets = []\n",
    "                for triplet in valid_triplets:\n",
    "                    head = triplet['h']\n",
    "                    tail = triplet['t']\n",
    "                    relations = triplet['r']\n",
    "                    conf = triplet['c']\n",
    "                    if conf < args.threshold:\n",
    "                        continue\n",
    "                    mapped_triplet = Map(head, relations, tail)\n",
    "                    if 'h' in mapped_triplet:\n",
    "                        mapped_triplet['c'] = conf\n",
    "                        mapped_triplets.append(mapped_triplet)\n",
    "                output = { 'line': idx, 'tri': deduplication(mapped_triplets) }\n",
    "\n",
    "                if include_sentence:\n",
    "                    output['sent'] = sentence\n",
    "                if len(output['tri']) > 0:\n",
    "                    g.write(json.dumps( output )+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp:\n",
    "    def __init__(self, sentence):\n",
    "        self.text = sentence\n",
    "        self.noun_chunks = []\n",
    "        \n",
    "def parse_sentence(*args, **kwargs):\n",
    "    \"\"\" parse sentence \"\"\"\n",
    "    inputs, tokenid2word_mapping, token2id, noun_chunks  = create_mapping(sentence, return_pt=True, nlp=nlp, tokenizer=tokenizer)\n",
    "    triplets = []\n",
    "    return triplets\n",
    "\n",
    "def create_mapping(sentence, return_pt, nlp, tokenizer):\n",
    "    inputs = None\n",
    "    tokenid2word = None\n",
    "    token2id = None\n",
    "    noun_chunks = None\n",
    "    return inputs, tokenid2word_mapping, token2id, noun_chunks\n",
    "\n",
    "tokenizer = None\n",
    "encoder = None\n",
    "\n",
    "sentence = 'Bob Dylan Ã¤r en gubbe som har skrivit musik.'\n",
    "\n",
    "triplets = parse_sentence(sent.text, tokenizer, encoder, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bob Dylan aer en gubbe som har skrivit musik.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_mapping(sentence, return_pt=True, nlp=nlp, tokenizer=tokenizer):\n",
    "    return None\n",
    "\n",
    "def parse_sentence(sentence, tokenizer, encoder, nlp, use_cuda=True):\n",
    "    ''' Parse stuff '''\n",
    "    tokenizer_name = str(tokenizer.__str__)\n",
    "\n",
    "    inputs, tokenid2word_mapping, token2id, noun_chunks  = create_mapping(sentence, return_pt=True, nlp=nlp, tokenizer=tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if use_cuda:\n",
    "            for key in inputs.keys():\n",
    "                inputs[key] = inputs[key].cuda()\n",
    "        outputs = encoder(**inputs, output_attentions=True)\n",
    "    trim = True\n",
    "    if 'GPT2' in tokenizer_name:\n",
    "        trim  = False\n",
    "\n",
    "    '''\n",
    "    Use average of last layer attention : page 6, section 3.1.2\n",
    "    '''\n",
    "    attention = process_matrix(outputs[2], avg_head=True, trim=trim, use_cuda=use_cuda)\n",
    "\n",
    "    merged_attention = compress_attention(attention, tokenid2word_mapping)\n",
    "    attn_graph = build_graph(merged_attention)\n",
    "\n",
    "    tail_head_pairs = []\n",
    "    for head in noun_chunks:\n",
    "        for tail in noun_chunks:\n",
    "            if head != tail:\n",
    "                tail_head_pairs.append((token2id[head], token2id[tail]))\n",
    "\n",
    "    black_list_relation = set([ token2id[n]  for n in noun_chunks ])\n",
    "\n",
    "    all_relation_pairs = []\n",
    "    id2token = { value: key for key, value in token2id.items()}\n",
    "\n",
    "    with Pool(10) as pool:\n",
    "        params = [  ( pair[0], pair[1], attn_graph, max(tokenid2word_mapping), black_list_relation, ) for pair in tail_head_pairs]\n",
    "        for output in pool.imap_unordered(bfs, params):\n",
    "            if len(output):\n",
    "                all_relation_pairs += [ (o, id2token) for o in output ]\n",
    "\n",
    "    triplet_text = []\n",
    "    with Pool(10, global_initializer, (nlp,)) as pool:\n",
    "        for triplet in pool.imap_unordered(filter_relation_sets, all_relation_pairs):\n",
    "            if len(triplet) > 0:\n",
    "                triplet_text.append(triplet)\n",
    "    return triplet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('xx_ent_wiki_sm')\n",
    "#nlp = spacy.load('da_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
